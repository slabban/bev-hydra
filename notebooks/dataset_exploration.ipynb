{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from lightning import LightningDataModule\n",
    "\n",
    "from src.data.components.data import FuturePredictionDataset\n",
    "from omegaconf import DictConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "common = OmegaConf.create({\n",
    "    'semantic_segmentation': {\n",
    "        'weights': [1.0, 2.0],\n",
    "        'use_top_k': True,\n",
    "        'top_k_ratio': 0.25,\n",
    "    },\n",
    "    'receptive_field': 3,\n",
    "    'future_discount': 0.95,\n",
    "    'ignore_index': 255,\n",
    "    'lift': {\n",
    "        'x_bound': [-50.0, 50.0, 0.5],\n",
    "        'y_bound': [-50.0, 50.0, 0.5],\n",
    "        'z_bound': [-10.0, 10.0, 20.0],\n",
    "        'd_bound': [2.0, 50.0, 1.0],\n",
    "    },\n",
    "    'image': {\n",
    "        'final_dim': [224, 480],\n",
    "        'resize_scale': 0.3,\n",
    "        'top_crop': 46,\n",
    "        'original_height': 900,\n",
    "        'original_width': 1600,\n",
    "        'names': ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'],\n",
    "    },\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_root = '/home/slabban/machine_learning/projects/bev-hydra/data/nuscenes'\n",
    "version = 'trainval'\n",
    "batch_size = 1\n",
    "filter_invisible_vehicles = True\n",
    "num_workers = 4\n",
    "pin_memory = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = FuturePredictionDataset(\n",
    "            data_root=data_root, is_train=True, version=version, batch_size=batch_size, filter_invisible_vehicles=filter_invisible_vehicles, common=common\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "            dataset=data_train,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            shuffle=True, drop_last= True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'intrinsics', 'extrinsics', 'segmentation', 'instance', 'centerness', 'offset', 'flow', 'future_egomotion', 'sample_token', 'z_position', 'attribute'])\n"
     ]
    }
   ],
   "source": [
    "print(sample.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1266e+00,  4.2961e-02,  2.5883e-02,  1.9745e-03, -2.6767e-03,\n",
      "         -4.7825e-03],\n",
      "        [-6.1031e+00,  7.6385e-02,  2.7347e-02, -1.0644e-03, -1.7784e-03,\n",
      "         -2.1298e-03],\n",
      "        [-5.0339e+00,  2.9167e-02,  1.5949e-02, -1.3499e-03, -1.3432e-03,\n",
      "         -4.2299e-04]])\n"
     ]
    }
   ],
   "source": [
    "for future_ego_pose in sample['future_egomotion']:\n",
    "    print(future_ego_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_all_camera_images_at_timestep(sample, timestep=0):\n",
    "    \"\"\"\n",
    "    Visualizes all camera images from a specific timestep of a given sample in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: instance of FuturePredictionDataset or similar.\n",
    "    - sample_index: Index of the sample to visualize.\n",
    "    - timestep: Timestep of the images to visualize.\n",
    "    \"\"\"\n",
    "    # Fetch the sample data\n",
    "    images_tensor = sample['image']  # Assuming shape is (T, N, 3, H, W)\n",
    "    \n",
    "    # Select all images at the given timestep\n",
    "    timestep_images = images_tensor[timestep]  # This selects all N camera images at the given timestep\n",
    "    \n",
    "    # Create a subplot for each camera image\n",
    "    num_cameras = timestep_images.shape[0]\n",
    "    fig, axs = plt.subplots(1, num_cameras, figsize=(15, 10))\n",
    "    \n",
    "    for i, img in enumerate(timestep_images):\n",
    "        # img is of shape (3, H, W), need to permute for plotting\n",
    "        axs[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "        axs[i].set_title(f'Camera {i+1}')\n",
    "        axs[i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_all_camera_images_at_timestep(dataset, sample_index=0, timestep=0)  # Adjust sample_index as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl-hydra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
